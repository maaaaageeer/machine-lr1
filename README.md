# Основы машинного обучения  
## Лабораторная работа №1: Линейная регрессия  



---

### Ход работы  

#### 1. Разделение данных на выборки  
- **Размер тестовой выборки:** 268 наблюдений.  

#### 2. Обучение модели линейной регрессии  
- **Свободный член (intercept):** -1236.  
- **Метрики на тренировочных данных:**  
  - R² = 0.751  
  - MAE = 4181  
  - MAPE = 43%  
- **Метрики на тестовых данных:**  
  - R² = 0.799  
  - MAE = 4189  
  - MAPE = 42%  

#### 3. Анализ Boxplot ошибок  
- **Наблюдения:**  
  - **A)** Разброс ошибок на тестовой выборке больше, чем на тренировочной.  
  - **D)** Медианная ошибка на обеих выборках положительная.  

#### 4. Min-Max нормализация и полиномиальные признаки  
- **Результат:** 105 признаков после преобразований.  

#### 5. Линейная регрессия на полиномиальных признаках  
- **R²:** 0.881 (улучшение по сравнению с базовой моделью).  

#### 6. Анализ коэффициентов полиномиальной модели  
- **Проблема:** Коэффициенты достигают значений ±1e15, что указывает на неустойчивость модели.  
- **Рекомендация:** Требуется регуляризация. ✅  

#### 7. Lasso-регрессия на полиномиальных признаках  
- **Метрики:**  
  - R² = 0.861  
  - MAE = 3056  
  - MAPE = 31%  
- **Вывод:** Lasso улучшила устойчивость модели, снизив ошибки (MAE, MAPE) по сравнению с линейной регрессией.  

--- 

**Итог:**  
Полиномиальные признаки повысили качество модели (R²), но без регуляризации коэффициенты становятся неустойчивыми. Lasso-регрессия устранила эту проблему, сохранив высокую точность.  
